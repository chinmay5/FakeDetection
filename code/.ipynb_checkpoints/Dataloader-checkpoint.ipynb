{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data1\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2cat(label_encoder, list):\n",
    "    return label_encoder.transform(list)\n",
    "\n",
    "def labels2onehot(OneHotEncoder, label_encoder, list):\n",
    "    return OneHotEncoder.transform(label_encoder.transform(list).reshape(-1, 1)).toarray()\n",
    "\n",
    "def onehot2labels(label_encoder, y_onehot):\n",
    "    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n",
    "\n",
    "def cat2labels(label_encoder, y_cat):\n",
    "    return label_encoder.inverse_transform(y_cat).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/chinmay/datatset/compressed_data/compressed23_10/train'\n",
    "labels = ['altered','original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_3DCNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, folders, labels, frames, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.labels = labels\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.folders)\n",
    "\n",
    "    def read_images(self, data_path, selected_folder, use_transform):\n",
    "        X = []\n",
    "        for i in self.frames:\n",
    "            image = Image.open(os.path.join(self.folders))\n",
    "\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            X.append(image.squeeze_(0))\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        folder = self.folders[index]\n",
    "\n",
    "        # Load data\n",
    "        X = self.read_images(data_path, folder, self.transform).unsqueeze_(0)                      # (input) spatial images\n",
    "        y = torch.from_numpy(np.array(self.labels[index])).type(torch.LongTensor)   # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        # print(X.shape)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset_3DCNN(data_path,labels,10)\n",
    "#print(data.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------ 3D CNN module ---------------------- ##\n",
    "def conv3D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv3D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),\n",
    "                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, t_dim=120, img_x=90, img_y=120, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=50):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        # set video dimension\n",
    "        self.t_dim = t_dim\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "        self.ch1, self.ch2 = 32, 48\n",
    "        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size\n",
    "        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n",
    "        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding\n",
    "\n",
    "        # compute conv1 & conv2 output shape\n",
    "        self.conv1_outshape = conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\n",
    "        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\n",
    "                               padding=self.pd1)\n",
    "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\n",
    "                               padding=self.pd2)\n",
    "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout3d(self.drop_p)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],\n",
    "                             self.fc_hidden1)  # fully connected hidden layer\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # fully connected layer, output = multi-classes\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        # Conv 1\n",
    "        x = self.conv1(x_3d)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # Conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # FC 1 and 2\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinmay/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/chinmay/Desktop/TUM/Sem-3/Advance-DL/fake-video-dataset/compressed23_10/train/altered'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cf6d04fd93c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mall_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'/home/chinmay/Desktop/TUM/Sem-3/Advance-DL/fake-video-dataset/compressed23_10/train/altered'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'altered'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/chinmay/Desktop/TUM/Sem-3/Advance-DL/fake-video-dataset/compressed23_10/train/altered'"
     ]
    }
   ],
   "source": [
    "# set path\n",
    "data_path = \"/home/chinmay/datatset/compressed_data/compressed23_10/saved_models\"    # define UCF-101 spatial data path\n",
    "action_name_path = \"./UCF101actions.pkl\"  # load preprocessed action names\n",
    "save_model_path = \"./Conv3D_ckpt/\"  # save Pytorch models\n",
    "\n",
    "\n",
    "# 3D CNN parameters\n",
    "fc_hidden1, fc_hidden2 = 256, 256\n",
    "dropout = 0.0        # dropout probability\n",
    "\n",
    "# training parameters\n",
    "k = 101            # number of target category\n",
    "epochs = 15\n",
    "batch_size = 30\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10\n",
    "img_x, img_y = 256, 342  # resize video 2d frame size\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 10, 1\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # output size = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y, reduction='elementwise_mean')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores\n",
    "\n",
    "\n",
    "# def validation(model, device, optimizer, test_loader):\n",
    "#     # set model as testing mode\n",
    "#     model.eval()\n",
    "\n",
    "#     test_loss = 0\n",
    "#     all_y = []\n",
    "#     all_y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in test_loader:\n",
    "#             # distribute data to device\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "\n",
    "#             output = model(X)\n",
    "\n",
    "#             loss = F.cross_entropy(output, y, reduction='sum')\n",
    "#             test_loss += loss.item()                 # sum up batch loss\n",
    "#             y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "#             # collect all y and y_pred in all batches\n",
    "#             all_y.extend(y)\n",
    "#             all_y_pred.extend(y_pred)\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     # to compute accuracy\n",
    "#     all_y = torch.stack(all_y, dim=0)\n",
    "#     all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "#     test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "#     # show information\n",
    "#     print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "#     # save Pytorch models of best record\n",
    "#     torch.save(model.state_dict(), os.path.join(save_model_path, '3dcnn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "#     torch.save(optimizer.state_dict(), os.path.join(save_model_path, '3dcnn_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "#     print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "#     return test_loss, test_score\n",
    "\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# load UCF101 actions names\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# load UCF101 actions names\n",
    "#with open(action_name_path, 'rb') as f:\n",
    "#    action_names = pickle.load(f)   # load UCF101 actions names\n",
    "\n",
    "\n",
    "action_names = ['altered', 'Original']\n",
    "# convert labels -> category\n",
    "le = LabelEncoder()\n",
    "le.fit(action_names)\n",
    "\n",
    "# show how many classes there are\n",
    "list(le.classes_)\n",
    "\n",
    "# convert category -> 1-hot\n",
    "action_category = le.transform(action_names).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(action_category)\n",
    "\n",
    "# # example\n",
    "# y = ['HorseRace', 'YoYo', 'WalkingWithDog']\n",
    "# y_onehot = labels2onehot(enc, le, y)\n",
    "# y2 = onehot2labels(le, y_onehot)\n",
    "\n",
    "\n",
    "actions = []\n",
    "\n",
    "\n",
    "all_names = []\n",
    "path= '/home/chinmay/datatset/compressed_data/compressed23_10/train/altered'\n",
    "fnames = os.listdir(path)\n",
    "for f in fnames:\n",
    "    actions.append('altered')\n",
    "\n",
    "    all_names.append(os.path.join(data_path, f))\n",
    "path1= '/home/chinmay/datatset/compressed_data/compressed23_10/train/original'\n",
    "fnames = os.listdir(path1)\n",
    "for f in fnames:\n",
    "    actions.append('original')\n",
    "\n",
    "    all_names.append(os.path.join(data_path, f))\n",
    "\n",
    "#print(all_names)\n",
    "\n",
    "# list all data files\n",
    "#all_X_list = all_names              # all video file names\n",
    "#all_y_list = labels2cat(le, actions)    # all video labels\n",
    "\n",
    "# train, test split\n",
    "train_list, test_list, train_label, test_label = train_test_split(all_names, actions, test_size=0.25, random_state=42)\n",
    "\n",
    "# image transformation\n",
    "transform = transforms.Compose([transforms.Resize([img_x, img_y]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "train_set, valid_set = Dataset_3DCNN(train_list, train_label, selected_frames, transform=transform), \\\n",
    "                       Dataset_3DCNN(test_list, test_label, selected_frames, transform=transform)\n",
    "train_loader = data1.DataLoader(train_set, **params)\n",
    "valid_loader = data1.DataLoader(valid_set, **params)\n",
    "\n",
    "# create model\n",
    "cnn3d = CNN3D(t_dim=len(selected_frames), img_x=img_x, img_y=img_y,\n",
    "              drop_p=dropout, fc_hidden1=fc_hidden1,  fc_hidden2=fc_hidden2, num_classes=k).to(device)\n",
    "\n",
    "\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn3d = nn.DataParallel(cnn3d)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn3d.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, cnn3d, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(cnn3d, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('./3DCNN_epoch_training_losses.npy', A)\n",
    "    np.save('./3DCNN_epoch_training_scores.npy', B)\n",
    "    np.save('./3DCNN_epoch_test_loss.npy', C)\n",
    "    np.save('./3DCNN_epoch_test_score.npy', D)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "plt.title(\"model loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "# 2nd figure\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1, epochs + 1), B[:, -1])  # train accuracy (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n",
    "# plt.plot(histories.losses_val)\n",
    "plt.title(\"training scores\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "title = \"./fig_UCF101_3DCNN.png\"\n",
    "plt.savefig(title, dpi=600)\n",
    "# plt.close(fig)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

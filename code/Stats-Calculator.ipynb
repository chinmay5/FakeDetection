{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CRNN\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, frame_length=10, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.transform = transform\n",
    "        #self.frames = frames\n",
    "        self.folders = data_path\n",
    "        self.frames = frame_length #For our case since we are computing 10 frames always\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.folders))\n",
    "\n",
    "    def read_images(self, data_path, use_transform):\n",
    "        X = []\n",
    "        file_name = \"\"\n",
    "        for i in os.listdir(data_path):\n",
    "            file_name = i\n",
    "            image = Image.open(os.path.join(data_path,i))\n",
    "            \n",
    "            #print(image.shape)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "                #print(image.size)\n",
    "            image = torch.from_numpy(np.asarray(image))\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X, file_name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.folders,os.listdir(self.folders)[index])\n",
    "              \n",
    "        # Load data\n",
    "        X, file_name = self.read_images(data_path, self.transform)                     # (input) spatial images\n",
    "        \n",
    "        y = np.ones(self.frames)\n",
    "        if 'real' in data_path:\n",
    "            y = np.zeros(self.frames)\n",
    "        #print(\"Folder is {}\".format(data_path))\n",
    "        #print(X.shape)\n",
    "        return X, torch.from_numpy(y).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/chinmay/datatset/train/'\n",
    "train_data = Dataset_CRNN(train_path, transform=TRANSFORM_IMG)\n",
    "# for step, (x, y) in enumerate(data):\n",
    "#     print(x.shape)\n",
    "#val_path = \"/home/chinmay/datatset/deepfake_split/val\"\n",
    "#val_data = Dataset_CRNN(val_path, transform=TRANSFORM_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=8,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n",
      "torch.Size([8, 10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (X, y) in enumerate(loader):\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n",
      "torch.Size([8, 3, 655360])\n"
     ]
    }
   ],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "for data_iter in loader:\n",
    "    data = data_iter[0]\n",
    "    batch_samples = data.size(0)\n",
    "    # get the data seize\n",
    "    # print(data.size())\n",
    "    data = data.view(batch_samples, data.size(2), -1)\n",
    "    print(data.size())\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean /= nb_samples\n",
    "std /= nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the sample is tensor([0.4246, 0.4144, 0.4114])\n",
      "Standard Deviation tensor([0.2265, 0.2208, 0.2215])\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of the sample is {}\".format(mean))\n",
    "print(\"Standard Deviation {}\".format(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    # Finding the Best Predicted image on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/chinmay/datatset/deepfake_split/val'\n",
    "val_data = Dataset_CRNN(val_path, transform=TRANSFORM_IMG, frame_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "params = {'batch_size': 8, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "valid_loader = data.DataLoader(val_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(numpy_array = []): #This is expected to take an array of array. So,\n",
    "    #print(\"Input array is {}\".format(numpy_array))\n",
    "    output = []\n",
    "    for array in numpy_array:\n",
    "        counts = np.bincount(array)\n",
    "        output.append(np.argmax(counts))\n",
    "    return torch.from_numpy(np.asarray(output)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    max_score = 0\n",
    "    max_label = ''\n",
    "    with torch.no_grad():\n",
    "        for X, y,file_name in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #X = X.permute(0,1,4,2,3)# Required to match shapes\n",
    "            #print(X.shape)\n",
    "            output = model(X)\n",
    "            #print(\"Output shape is {}\".format(output.shape))\n",
    "            #print(\"Expected shape is {}\".format(y.shape))\n",
    "            for items in range(output.shape[1]):\n",
    "                loss = loss_function(output[:,items,:], y[:,items])\n",
    "                test_loss += loss.item()                 # sum up batch loss\n",
    "            #Predict value\n",
    "            \n",
    "            y_pred = torch.max(output, 2)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "            # Replace the value with the median value\n",
    "            possible_scores = torch.max(output, 2)[0]\n",
    "            #print(possible_scores)\n",
    "            for score_arr in possible_scores:\n",
    "                for score in score_arr:\n",
    "                    if score > max_score:\n",
    "                        print(\"score array is {}\".format(score_arr))\n",
    "                        print(\"And filename is {}\".format(file_name))\n",
    "                        max_score = score     \n",
    "                        #index = X[score_arr == score] #Getting from X the value we wanted\n",
    "                        # max_label = index\n",
    "            y_pred = find_median(y_pred)\n",
    "            y = find_median(y)\n",
    "            print(\"actual value of labels read from disc is {}\".format(y))\n",
    "            print(\"filename is {} and prediction is {}\".format(file_name, y_pred))\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "    print(\"maximum score is {}\".format(max_score))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "#     all_y = torch.stack(all_y, dim=0)\n",
    "#     all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "#     test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "#     # show information\n",
    "#     print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    #   torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "#     print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return 0, 0 #Immaterial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Meso4 import Meso4\n",
    "model = Meso4()\n",
    "\n",
    "# Load data from the specified location\n",
    "save_path = \"/home/chinmay/visualisation/pytorch-cnn-visualizations/src/\"\n",
    "PATH = save_path + \"meso_df_2_epoch28.pth\"\n",
    " \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.cuda()\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array is tensor([2.5441, 2.9198, 1.6562, 2.4362, 2.4476, 2.9700, 2.0143, 2.3608, 1.9688,\n",
      "        0.9938, 1.8933, 2.3053, 1.6846, 1.1032, 0.1836, 2.2953, 1.2419, 0.5873,\n",
      "        2.3343, 0.3434, 1.4414, 2.3553, 2.5622, 1.7238, 0.3480, 0.8488, 2.5494,\n",
      "        2.0040, 1.3833, 0.2105], device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([2.5441, 2.9198, 1.6562, 2.4362, 2.4476, 2.9700, 2.0143, 2.3608, 1.9688,\n",
      "        0.9938, 1.8933, 2.3053, 1.6846, 1.1032, 0.1836, 2.2953, 1.2419, 0.5873,\n",
      "        2.3343, 0.3434, 1.4414, 2.3553, 2.5622, 1.7238, 0.3480, 0.8488, 2.5494,\n",
      "        2.0040, 1.3833, 0.2105], device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([2.5441, 2.9198, 1.6562, 2.4362, 2.4476, 2.9700, 2.0143, 2.3608, 1.9688,\n",
      "        0.9938, 1.8933, 2.3053, 1.6846, 1.1032, 0.1836, 2.2953, 1.2419, 0.5873,\n",
      "        2.3343, 0.3434, 1.4414, 2.3553, 2.5622, 1.7238, 0.3480, 0.8488, 2.5494,\n",
      "        2.0040, 1.3833, 0.2105], device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([ 3.0243,  2.5252,  4.6965,  1.2653,  1.1415,  4.2684,  0.3159,  2.0393,\n",
      "         0.2174,  4.3667,  2.1883,  2.3273,  1.0465, -0.0568,  1.5078,  1.1648,\n",
      "         0.1307,  2.5103,  2.1636,  3.7318,  1.0862,  1.1652,  0.0402,  2.5420,\n",
      "         3.2105,  0.7636,  3.0379,  2.6558,  0.1866,  0.8903],\n",
      "       device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([ 3.0243,  2.5252,  4.6965,  1.2653,  1.1415,  4.2684,  0.3159,  2.0393,\n",
      "         0.2174,  4.3667,  2.1883,  2.3273,  1.0465, -0.0568,  1.5078,  1.1648,\n",
      "         0.1307,  2.5103,  2.1636,  3.7318,  1.0862,  1.1652,  0.0402,  2.5420,\n",
      "         3.2105,  0.7636,  3.0379,  2.6558,  0.1866,  0.8903],\n",
      "       device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([2.0022, 4.3001, 0.8782, 2.1920, 2.5082, 1.0061, 2.8004, 1.4296, 1.5981,\n",
      "        0.8643, 4.8982, 5.7581, 1.4156, 1.4383, 2.0388, 4.2848, 1.2974, 0.7740,\n",
      "        1.5614, 2.6126, 2.9269, 1.9095, 1.0627, 3.0716, 1.1261, 0.8061, 0.7715,\n",
      "        4.8364, 0.7782, 1.8193], device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "score array is tensor([2.0022, 4.3001, 0.8782, 2.1920, 2.5082, 1.0061, 2.8004, 1.4296, 1.5981,\n",
      "        0.8643, 4.8982, 5.7581, 1.4156, 1.4383, 2.0388, 4.2848, 1.2974, 0.7740,\n",
      "        1.5614, 2.6126, 2.9269, 1.9095, 1.0627, 3.0716, 1.1261, 0.8061, 0.7715,\n",
      "        4.8364, 0.7782, 1.8193], device='cuda:0')\n",
      "And filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg']\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 1, 0, 0, 1])\n",
      "filename is ['132_228.jpg', '78_828.jpg', '58_138.jpg', '92_714.jpg', '128_1122.jpg', '68_664.jpg', '99_298.jpg', '175_12.jpg'] and prediction is tensor([1, 0, 0, 0, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([0, 1, 0, 1, 1, 0, 1, 0])\n",
      "filename is ['73_132.jpg', '120_725.jpg', '64_438.jpg', '143_7.jpg', '176_48.jpg', '76_243.jpg', '127_412.jpg', '38_516.jpg'] and prediction is tensor([0, 1, 0, 0, 0, 0, 1, 0])\n",
      "score array is tensor([ 0.2490,  5.8531,  1.3180,  0.8591,  1.2698,  2.3740,  1.5601,  1.3166,\n",
      "         1.0303,  1.4363,  1.7339,  1.0961,  5.3872,  1.0839,  1.1838,  1.3691,\n",
      "         1.0220,  1.2902,  1.2331,  1.6963,  6.9930, -0.0115,  1.2202,  1.5730,\n",
      "         1.3062,  0.8204,  1.1234,  0.7907,  0.4063,  0.8740],\n",
      "       device='cuda:0')\n",
      "And filename is ['94_267.jpg', '159_210.jpg', '118_171.jpg', '41_132.jpg', '116_296.jpg', '166_89.jpg', '172_350.jpg', '135_1200.jpg']\n",
      "score array is tensor([ 0.2490,  5.8531,  1.3180,  0.8591,  1.2698,  2.3740,  1.5601,  1.3166,\n",
      "         1.0303,  1.4363,  1.7339,  1.0961,  5.3872,  1.0839,  1.1838,  1.3691,\n",
      "         1.0220,  1.2902,  1.2331,  1.6963,  6.9930, -0.0115,  1.2202,  1.5730,\n",
      "         1.3062,  0.8204,  1.1234,  0.7907,  0.4063,  0.8740],\n",
      "       device='cuda:0')\n",
      "And filename is ['94_267.jpg', '159_210.jpg', '118_171.jpg', '41_132.jpg', '116_296.jpg', '166_89.jpg', '172_350.jpg', '135_1200.jpg']\n",
      "actual value of labels read from disc is tensor([0, 1, 1, 0, 1, 1, 1, 1])\n",
      "filename is ['94_267.jpg', '159_210.jpg', '118_171.jpg', '41_132.jpg', '116_296.jpg', '166_89.jpg', '172_350.jpg', '135_1200.jpg'] and prediction is tensor([0, 1, 1, 0, 1, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 0, 0, 1, 0])\n",
      "filename is ['144_66.jpg', '95_244.jpg', '73_732.jpg', '97_259.jpg', '66_21.jpg', '87_620.jpg', '170_1378.jpg', '96_122.jpg'] and prediction is tensor([1, 0, 0, 0, 0, 0, 1, 0])\n",
      "actual value of labels read from disc is tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
      "filename is ['97_62.jpg', '164_112.jpg', '77_534.jpg', '155_425.jpg', '81_182.jpg', '134_330.jpg', '101_474.jpg', '62_30.jpg'] and prediction is tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
      "actual value of labels read from disc is tensor([0, 1, 0, 1, 0, 0, 0, 1])\n",
      "filename is ['62_612.jpg', '152_280.jpg', '70_234.jpg', '145_280.jpg', '42_180.jpg', '79_375.jpg', '67_144.jpg', '122_38.jpg'] and prediction is tensor([0, 1, 0, 1, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
      "filename is ['72_340.jpg', '60_426.jpg', '69_196.jpg', '122_0.jpg', '71_214.jpg', '69_30.jpg', '116_896.jpg', '65_132.jpg'] and prediction is tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 1, 0, 0, 0])\n",
      "filename is ['135_800.jpg', '101_82.jpg', '96_320.jpg', '37_384.jpg', '114_570.jpg', '95_580.jpg', '55_78.jpg', '83_57.jpg'] and prediction is tensor([1, 0, 0, 0, 1, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([0, 0, 1, 1, 0, 0, 0, 0])\n",
      "filename is ['79_93.jpg', '49_1410.jpg', '142_176.jpg', '128_816.jpg', '34_78.jpg', '45_48.jpg', '84_474.jpg', '74_188.jpg'] and prediction is tensor([0, 0, 1, 1, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
      "filename is ['149_3.jpg', '40_12.jpg', '88_253.jpg', '89_490.jpg', '98_178.jpg', '120_110.jpg', '170_234.jpg', '102_254.jpg'] and prediction is tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 0, 1, 0, 0, 0, 0, 0])\n",
      "filename is ['148_178.jpg', '42_276.jpg', '150_1495.jpg', '82_381.jpg', '85_114.jpg', '77_210.jpg', '55_126.jpg', '63_507.jpg'] and prediction is tensor([1, 0, 1, 0, 0, 0, 0, 0])\n",
      "score array is tensor([0.0433, 5.2399, 0.6164, 4.6811, 0.4112, 0.2913, 6.4078, 6.1796, 0.2159,\n",
      "        2.5524, 4.4354, 1.6039, 3.9252, 3.6617, 3.8932, 0.7746, 0.5772, 3.6808,\n",
      "        1.7166, 4.2774, 7.8604, 6.9359, 1.4299, 3.2039, 6.2026, 0.8651, 0.4875,\n",
      "        0.2648, 1.0504, 4.6049], device='cuda:0')\n",
      "And filename is ['70_579.jpg', '70_342.jpg', '121_140.jpg', '93_365.jpg', '43_198.jpg', '128_1530.jpg', '85_242.jpg', '173_67.jpg']\n",
      "actual value of labels read from disc is tensor([0, 0, 1, 0, 0, 1, 0, 1])\n",
      "filename is ['70_579.jpg', '70_342.jpg', '121_140.jpg', '93_365.jpg', '43_198.jpg', '128_1530.jpg', '85_242.jpg', '173_67.jpg'] and prediction is tensor([0, 0, 1, 0, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 0, 1, 0, 1, 1, 0])\n",
      "filename is ['126_300.jpg', '157_171.jpg', '90_895.jpg', '132_93.jpg', '77_447.jpg', '168_1250.jpg', '133_327.jpg', '100_76.jpg'] and prediction is tensor([1, 1, 0, 1, 0, 1, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 0, 1, 0, 0, 0, 1])\n",
      "filename is ['143_39.jpg', '176_150.jpg', '96_154.jpg', '123_106.jpg', '76_555.jpg', '67_354.jpg', '75_513.jpg', '161_172.jpg'] and prediction is tensor([1, 0, 0, 1, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([0, 0, 0, 0, 0, 1, 0, 1])\n",
      "filename is ['89_135.jpg', '65_218.jpg', '60_504.jpg', '75_273.jpg', '89_670.jpg', '158_291.jpg', '56_84.jpg', '124_186.jpg'] and prediction is tensor([0, 0, 0, 0, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "filename is ['115_6.jpg', '84_273.jpg', '87_900.jpg', '86_362.jpg', '78_741.jpg', '88_40.jpg', '67_534.jpg', '48_240.jpg'] and prediction is tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 0, 1, 0, 0, 1])\n",
      "filename is ['163_188.jpg', '152_316.jpg', '146_252.jpg', '49_444.jpg', '175_68.jpg', '57_66.jpg', '53_360.jpg', '162_128.jpg'] and prediction is tensor([1, 1, 1, 0, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 0, 0, 0, 0])\n",
      "filename is ['125_2.jpg', '164_82.jpg', '148_0.jpg', '168_110.jpg', '88_161.jpg', '82_96.jpg', '76_138.jpg', '96_66.jpg'] and prediction is tensor([1, 1, 1, 1, 1, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "filename is ['91_252.jpg', '44_72.jpg', '100_824.jpg', '94_552.jpg', '65_82.jpg', '74_816.jpg', '91_480.jpg', '77_99.jpg'] and prediction is tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "score array is tensor([1.9229, 3.0954, 1.2575, 0.9865, 4.7985, 1.7670, 3.2998, 0.1699, 2.5417,\n",
      "        0.9812, 3.8282, 3.3973, 2.9475, 2.5533, 2.6820, 5.8479, 1.4786, 6.2456,\n",
      "        3.0897, 2.8407, 0.8087, 2.1288, 5.5225, 2.8424, 6.4941, 5.3329, 2.9497,\n",
      "        6.9142, 2.7382, 8.1381], device='cuda:0')\n",
      "And filename is ['51_72.jpg', '85_298.jpg', '134_405.jpg', '72_196.jpg', '54_408.jpg', '48_12.jpg', '82_228.jpg', '165_393.jpg']\n",
      "actual value of labels read from disc is tensor([0, 0, 1, 0, 0, 0, 0, 1])\n",
      "filename is ['51_72.jpg', '85_298.jpg', '134_405.jpg', '72_196.jpg', '54_408.jpg', '48_12.jpg', '82_228.jpg', '165_393.jpg'] and prediction is tensor([0, 0, 1, 0, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 1, 0, 1, 0, 1])\n",
      "filename is ['167_212.jpg', '44_0.jpg', '102_42.jpg', '161_58.jpg', '90_1370.jpg', '118_309.jpg', '86_150.jpg', '169_864.jpg'] and prediction is tensor([1, 1, 0, 1, 0, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 0, 0, 1, 1])\n",
      "filename is ['126_565.jpg', '63_444.jpg', '91_776.jpg', '81_68.jpg', '64_148.jpg', '71_344.jpg', '142_120.jpg', '174_316.jpg'] and prediction is tensor([1, 0, 0, 0, 0, 0, 1, 1])\n",
      "actual value of labels read from disc is tensor([0, 1, 0, 1, 1, 0, 0, 1])\n",
      "filename is ['51_174.jpg', '144_108.jpg', '54_228.jpg', '154_50.jpg', '156_1221.jpg', '47_246.jpg', '100_4.jpg', '147_108.jpg'] and prediction is tensor([0, 1, 0, 1, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 0, 0, 0, 0, 0])\n",
      "filename is ['146_438.jpg', '168_90.jpg', '153_2.jpg', '61_1290.jpg', '94_195.jpg', '66_618.jpg', '39_66.jpg', '93_1145.jpg'] and prediction is tensor([1, 1, 1, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 0, 0, 0, 1, 0, 0, 0])\n",
      "filename is ['123_92.jpg', '37_150.jpg', '35_84.jpg', '45_168.jpg', '117_12.jpg', '68_148.jpg', '68_720.jpg', '38_288.jpg'] and prediction is tensor([1, 0, 0, 0, 1, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([0, 1, 0, 1, 1, 1, 1, 1])\n",
      "filename is ['83_393.jpg', '130_11.jpg', '80_498.jpg', '114_396.jpg', '158_33.jpg', '149_117.jpg', '130_7.jpg', '140_465.jpg'] and prediction is tensor([0, 0, 0, 1, 1, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([0, 1, 1, 0, 1, 1, 1, 0])\n",
      "filename is ['50_96.jpg', '155_260.jpg', '162_86.jpg', '73_436.jpg', '131_92.jpg', '125_110.jpg', '151_280.jpg', '72_130.jpg'] and prediction is tensor([0, 1, 1, 0, 1, 1, 1, 0])\n",
      "actual value of labels read from disc is tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
      "filename is ['58_66.jpg', '119_580.jpg', '157_594.jpg', '99_140.jpg', '160_594.jpg', '86_316.jpg', '79_459.jpg', '173_77.jpg'] and prediction is tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 0, 0, 0, 0, 0])\n",
      "filename is ['165_33.jpg', '159_3.jpg', '172_600.jpg', '98_57.jpg', '101_266.jpg', '63_276.jpg', '40_54.jpg', '92_1056.jpg'] and prediction is tensor([1, 1, 1, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([0, 1, 1])\n",
      "filename is ['52_270.jpg', '150_26.jpg', '153_6.jpg'] and prediction is tensor([0, 1, 1])\n",
      "maximum score is 8.138072967529297\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    # train_losses, train_scores = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(model, 'cuda', valid_loader)\n",
    "    # Reduce learning-rate by a factor of 1/10 after every 10 epochs\n",
    "    # avoid this step as Adam is being used\n",
    "    #adjust_learning_rate(optimizer=optimizer, learning_rate=learning_rate, epoch=epoch)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader for the faceForensic datatset to see the best image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/chinmay/datatset/val'\n",
    "val_data = Dataset_CRNN(val_path, transform=TRANSFORM_IMG, frame_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Meso4 import Meso4\n",
    "model = Meso4()\n",
    "\n",
    "# Load data from the specified location\n",
    "save_path = \"/home/chinmay/visualisation/pytorch-cnn-visualizations/src/\"\n",
    "PATH = save_path + \"meso_f2f_epoch28.pth\"\n",
    " \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.cuda()\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "params = {'batch_size': 8, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "valid_loader = data.DataLoader(val_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array is tensor([0.9196, 1.0822, 2.3597, 0.8591, 1.3110, 2.2409, 0.7416, 0.1697, 0.5236,\n",
      "        0.3696], device='cuda:0')\n",
      "And filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_3.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_6.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_original_input_8.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_original_input_6.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_6.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_original_input_7.png', 's8raRFJ7z9w_0_sqfDnCME350_0_9.png']\n",
      "score array is tensor([0.9196, 1.0822, 2.3597, 0.8591, 1.3110, 2.2409, 0.7416, 0.1697, 0.5236,\n",
      "        0.3696], device='cuda:0')\n",
      "And filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_3.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_6.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_original_input_8.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_original_input_6.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_6.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_original_input_7.png', 's8raRFJ7z9w_0_sqfDnCME350_0_9.png']\n",
      "score array is tensor([0.9196, 1.0822, 2.3597, 0.8591, 1.3110, 2.2409, 0.7416, 0.1697, 0.5236,\n",
      "        0.3696], device='cuda:0')\n",
      "And filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_3.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_6.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_original_input_8.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_original_input_6.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_6.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_original_input_7.png', 's8raRFJ7z9w_0_sqfDnCME350_0_9.png']\n",
      "score array is tensor([1.3675, 2.0023, 1.6827, 1.0469, 0.2793, 2.5332, 0.1702, 0.1134, 2.0545,\n",
      "        0.7565], device='cuda:0')\n",
      "And filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_3.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_6.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_original_input_8.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_original_input_6.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_6.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_original_input_7.png', 's8raRFJ7z9w_0_sqfDnCME350_0_9.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_3.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_6.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_original_input_8.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_original_input_6.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_6.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_original_input_7.png', 's8raRFJ7z9w_0_sqfDnCME350_0_9.png'] and prediction is tensor([0, 1, 1, 0, 0, 1, 1, 1])\n",
      "score array is tensor([2.0717, 2.2668, 2.3903, 1.8757, 2.5165, 1.7597, 1.4666, 2.9466, 2.3678,\n",
      "        2.8992], device='cuda:0')\n",
      "And filename is ['pRrc0B9FlOU_4_rGszRN55VHs_3_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_original_input_5.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_5.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_original_input_9.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_original_input_4.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_original_input_4.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_original_input_1.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_original_input_6.png']\n",
      "score array is tensor([2.5899, 3.4496, 2.8291, 0.7569, 2.6436, 3.4634, 3.5504, 3.3032, 2.9465,\n",
      "        1.7937], device='cuda:0')\n",
      "And filename is ['pRrc0B9FlOU_4_rGszRN55VHs_3_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_original_input_5.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_5.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_original_input_9.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_original_input_4.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_original_input_4.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_original_input_1.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_original_input_6.png']\n",
      "score array is tensor([2.5899, 3.4496, 2.8291, 0.7569, 2.6436, 3.4634, 3.5504, 3.3032, 2.9465,\n",
      "        1.7937], device='cuda:0')\n",
      "And filename is ['pRrc0B9FlOU_4_rGszRN55VHs_3_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_original_input_5.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_5.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_original_input_9.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_original_input_4.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_original_input_4.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_original_input_1.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_original_input_6.png']\n",
      "score array is tensor([2.5899, 3.4496, 2.8291, 0.7569, 2.6436, 3.4634, 3.5504, 3.3032, 2.9465,\n",
      "        1.7937], device='cuda:0')\n",
      "And filename is ['pRrc0B9FlOU_4_rGszRN55VHs_3_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_original_input_5.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_5.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_original_input_9.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_original_input_4.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_original_input_4.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_original_input_1.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_original_input_6.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['pRrc0B9FlOU_4_rGszRN55VHs_3_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_original_input_5.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_5.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_original_input_9.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_original_input_4.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_original_input_4.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_original_input_1.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_original_input_6.png'] and prediction is tensor([1, 0, 1, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['wedLZCKQSrM_0_w2h4xISxMtc_2_original_input_7.png', 'yueeQKJK-wc_0_xG9J__APvnY_4_2.png', 'wAoDSioZkV8_0_wbLsNxqHyeA_1_7.png', 'ukdxw_rACaw_0_ueWSYARSUnI_0_original_input_8.png', 'w-3xrMKdXBs_0_vviBDy30sXE_0_original_input_0.png', 'xZjp2wTRB00_2_xf9OCpcjNxw_1_6.png', 'rHABPG_ylvQ_3_r4evQe47qE0_3_original_input_9.png', 'sNCVZ7VmJ98_1_sNCVZ7VmJ98_1_original_input_1.png'] and prediction is tensor([1, 1, 0, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['y67jlPSTm2Y_3_ybIKgqf-Zls_0_3.png', 'xG9J__APvnY_0_yIvIfkuWRSY_1_2.png', 'zlKkyKw7xfw_0_yzL60Qro0Ec_0_original_input_5.png', 'ualXuiy2SN4_0_wlSJLDlCxlY_2_original_input_6.png', 'pl5Ru8opH9M_0_qpBoMZ8mAiQ_2_6.png', 'sO1UU5w51-4_1_tcyOetKW0BE_3_original_input_3.png', 'vOJztGgaiaY_1_vOTS31zUyBg_0_7.png', 'wedLZCKQSrM_2_uvxJpaN72WQ_2_original_input_3.png'] and prediction is tensor([1, 0, 0, 0, 0, 0, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['u1L10BSRzVs_2_wERnQNuwQbM_2_5.png', 'tuAgtClLSxE_0_tKVEzPMy9x0_0_4.png', 'uzWFtZBcOIA_3_uafR2rn51YA_0_original_input_5.png', 'wedLZCKQSrM_2_uvxJpaN72WQ_2_7.png', 'sA9vNgx6X-k_2_sgoA6ahEvjw_0_original_input_7.png', 'rgkJiQbfE8Q_3_rPBfL0TlQIc_4_9.png', 'wAu_YWTQ3Po_0_sRASRj8cG9E_2_original_input_8.png', '0GvV_83Wlf8_2_aqHwzt9uceI_0_original_input_3.png'] and prediction is tensor([1, 1, 1, 1, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['vm5DPRNMfvQ_0_uE-bz8xv0Mw_0_5.png', 'rBgM58w-gvM_1_pd8-LGKVGWQ_3_9.png', 'wuJZuJPVRUk_0_xQHrfDT6XJ0_4_5.png', 'ta1sycJ415M_1_xtYnIghWxJ0_0_3.png', 'zRUomg6M3j0_0_zPdvu-SUnbg_1_original_input_0.png', 'uuzCB0gt8kM_0_wnx2fsN9WP0_1_original_input_5.png', 'pl5Ru8opH9M_0_qpBoMZ8mAiQ_2_original_input_2.png', 'sZDvg03saDA_4_sZDvg03saDA_4_8.png'] and prediction is tensor([1, 1, 1, 1, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['viq-Tn_w1MM_0_wCIDKwgcpfw_0_3.png', 'sZ8hFchh-i8_3_saQyF1FPiqw_0_original_input_4.png', 'ydPPnou6Frs_2_xZjp2wTRB00_3_0.png', 'rWN7jc2SRCw_0_t0YYLT7MEW4_0_original_input_9.png', 'ukdxw_rACaw_4_wTsoXbhr0N8_4_original_input_6.png', 'tH6p0SPG5IM_0_tPT-6R-a1W0_0_3.png', 'ueWSYARSUnI_0_uvbyZZFC25g_2_original_input_3.png', 'xjYnDXIVPNo_2_xHTfDsJrI9g_0_original_input_9.png'] and prediction is tensor([1, 1, 1, 1, 1, 1, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yA9PpKq7EMY_1_y4PqkuhOzNc_1_6.png', 'ptJq_5_lF6Q_0_qZ_ozYF3eug_0_9.png', '0qjxH8dxbFE_0_9W5-B_oWec8_3_1.png', 'uvxJpaN72WQ_2_u80gOsgFcIs_4_original_input_3.png', 'wuJZuJPVRUk_0_xQHrfDT6XJ0_4_original_input_0.png', 'sx-ll4lW-o0_4_sO1UU5w51-4_0_original_input_4.png', 'vPFPOK5o3j0_0_wF2J3IRCd58_0_original_input_0.png', 'vPFPOK5o3j0_0_wF2J3IRCd58_0_7.png'] and prediction is tensor([1, 1, 1, 0, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['unUjJkjyh9E_2_wLkWLkZTEAU_1_3.png', 'vzPhIF7L2pg_1_uKOsEVyx17Y_0_6.png', 'wlSJLDlCxlY_2_wlSJLDlCxlY_2_original_input_2.png', 'yXdpfHzOVac_9_yZVyZN2kZKY_0_original_input_9.png', 'rIiQLEa8iZ0_0_rI7GbgEhwoY_1_2.png', 'pbuuZv60C4Q_6_r81Q3Gnrihc_1_original_input_3.png', 'rai6unYUpiM_0_rai6unYUpiM_0_original_input_1.png', 'xgW0lJJsGmc_0_xs-QgU_Msy0_0_original_input_8.png'] and prediction is tensor([1, 1, 0, 1, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['wUU-Y1ffMp0_0_uvxJpaN72WQ_2_2.png', 'qwB6fx8caeY_0_qb_sn9hVqh0_0_2.png', 'snQZfppXZeQ_0_ta1sycJ415M_2_original_input_3.png', 'zYtJwZWcRXk_3_zR3A0fzf1Kg_1_original_input_9.png', 'yq2rKzQrYVg_2_yX3GBqrETss_1_original_input_8.png', 'tuAgtClLSxE_2_s8CuBT-MDYE_4_8.png', 'sZ8hFchh-i8_3_saQyF1FPiqw_0_3.png', 't2T_pvkLm3s_1_s8CuBT-MDYE_4_4.png'] and prediction is tensor([1, 1, 0, 0, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['szxR2kMIWtI_0_saQyF1FPiqw_0_3.png', 'viq-Tn_w1MM_0_wCIDKwgcpfw_0_original_input_7.png', 'zRUomg6M3j0_0_zPdvu-SUnbg_1_9.png', 'yuyl1f-B6iU_0_y4PqkuhOzNc_1_original_input_7.png', 'r4evQe47qE0_4_rGszRN55VHs_3_original_input_4.png', 'vRCCZzmv5xs_1_ugyYHAxIMrc_0_7.png', 'x4dffEgdvwU_1_yX3GBqrETss_1_3.png', 'zA9-nGZacyg_0_zD_OLZWppaA_1_8.png'] and prediction is tensor([1, 0, 1, 0, 0, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['vJv2Bemw7MU_4_r0WUkAPxFhw_8_original_input_8.png', 'r0WUkAPxFhw_8_wAu_YWTQ3Po_0_original_input_2.png', 'w9H1lP-eOoA_3_wedLZCKQSrM_2_3.png', 'xffuIgiRUZw_6_ySy2dIJ889c_0_original_input_3.png', 'qZt442xDsAw_0_r4evQe47qE0_3_original_input_0.png', 'szxR2kMIWtI_1_tTzFkD7YJ8g_2_6.png', 'zIowgsPebcE_7_zR3A0fzf1Kg_1_original_input_4.png', 'xG9J__APvnY_8_x99NFdLvAIQ_1_original_input_1.png'] and prediction is tensor([0, 0, 1, 0, 0, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['y67jlPSTm2Y_3_ybIKgqf-Zls_0_original_input_2.png', 's8CuBT-MDYE_4_sZqIjquRfck_8_9.png', 'unUjJkjyh9E_2_wLkWLkZTEAU_1_original_input_6.png', 's1jqbrlDefs_0_tH6p0SPG5IM_0_original_input_1.png', 'ta1sycJ415M_1_xtYnIghWxJ0_0_original_input_6.png', 'vOR4lA-aOfI_1_unUjJkjyh9E_2_3.png', 'rBgM58w-gvM_1_pd8-LGKVGWQ_3_original_input_4.png', 'qpe2zydDOBM_0_r4evQe47qE0_4_4.png'] and prediction is tensor([0, 1, 0, 0, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['sgoA6ahEvjw_0_sqfDnCME350_0_5.png', 'snQZfppXZeQ_0_ta1sycJ415M_2_9.png', 'r4evQe47qE0_3_qQUcjcS06IY_1_1.png', 'rKi2MncpSAA_1_twX4w9Ti2bk_2_original_input_7.png', 'w-3xrMKdXBs_0_vviBDy30sXE_0_4.png', 'wLkWLkZTEAU_1_wAoDSioZkV8_0_5.png', 'ybIKgqf-Zls_0_y1uIt39-154_2_8.png', 'zlKkyKw7xfw_1_zRUomg6M3j0_0_original_input_7.png'] and prediction is tensor([1, 1, 1, 0, 1, 1, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['y_vhh1BuwDQ_0_wuJZuJPVRUk_0_6.png', '18KtBf-4sPs_0_5b96N4ciJRY_3_original_input_1.png', 'yj04mRa-AMA_1_yX3GBqrETss_1_9.png', 'w9H1lP-eOoA_0_wb2iK723AkI_3_2.png', 'uzWFtZBcOIA_3_uafR2rn51YA_0_8.png', 'rWN7jc2SRCw_0_t0YYLT7MEW4_0_1.png', 'wOBTdvyWWNk_0_uzWFtZBcOIA_3_3.png', 'ydPPnou6Frs_1_yplO7uWY1EM_2_original_input_7.png'] and prediction is tensor([1, 0, 1, 1, 1, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['xiGGcoQG8JI_0_ysNczfg7n8w_0_original_input_5.png', 'uafR2rn51YA_0_vJv2Bemw7MU_4_9.png', 'x5ycoTaWsns_1_yXdpfHzOVac_9_5.png', 'vzPhIF7L2pg_1_uKOsEVyx17Y_0_original_input_6.png', 'yXdpfHzOVac_6_xs-QgU_Msy0_0_0.png', 'x5ycoTaWsns_1_yXdpfHzOVac_9_original_input_5.png', 'ydPPnou6Frs_1_yplO7uWY1EM_2_3.png', 'wERnQNuwQbM_3_vOR4lA-aOfI_1_5.png'] and prediction is tensor([0, 1, 1, 0, 1, 0, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['v4JKM1tSlJg_2_w9H1lP-eOoA_0_3.png', 'xG9J__APvnY_8_x99NFdLvAIQ_1_8.png', 'zlKkyKw7xfw_1_zRUomg6M3j0_0_9.png', 'xG9J__APvnY_4_yNtEPE_OCfg_0_1.png', 'pSc8pCb5ug0_1_rIiQLEa8iZ0_2_1.png', 'rKi2MncpSAA_1_twX4w9Ti2bk_2_6.png', 'xjYnDXIVPNo_2_xHTfDsJrI9g_0_1.png', 'zYtJwZWcRXk_5_yyMFVp5Ut0Y_8_2.png'] and prediction is tensor([1, 1, 1, 1, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['sO1UU5w51-4_0_tTzFkD7YJ8g_2_9.png', 'yarXrvfBZ9c_0_xQHrfDT6XJ0_4_original_input_7.png', 'sgoA6ahEvjw_0_sqfDnCME350_0_original_input_6.png', 'pwsP8rk2ouU_1_qb_sn9hVqh0_0_7.png', 'wF2J3IRCd58_0_ukdxw_rACaw_0_original_input_7.png', 'xgIs6mU6klQ_2_yZVyZN2kZKY_0_3.png', 'wERnQNuwQbM_2_wAoDSioZkV8_0_7.png', 'tcyOetKW0BE_3_to6qdPy8ydg_9_7.png'] and prediction is tensor([1, 0, 0, 1, 0, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['xe1sO7aw6J4_7_y1uIt39-154_0_9.png', 'ukdxw_rACaw_3_vm5DPRNMfvQ_0_8.png', 'ta1sycJ415M_2_sZqIjquRfck_8_0.png', 'rHABPG_ylvQ_3_r4evQe47qE0_3_3.png', 'tuAgtClLSxE_2_s8CuBT-MDYE_4_original_input_9.png', 'sA9vNgx6X-k_2_sgoA6ahEvjw_0_2.png', 'yNtEPE_OCfg_0_y_vhh1BuwDQ_0_3.png', 'yuyl1f-B6iU_1_y8arX_WtdfM_1_1.png'] and prediction is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['zlKkyKw7xfw_0_yzL60Qro0Ec_0_3.png', 'tcyOetKW0BE_3_to6qdPy8ydg_9_original_input_7.png', '0GvV_83Wlf8_2_aqHwzt9uceI_0_7.png', 'yA13FQmjfNQ_0_yjsNEL7DQuY_5_3.png', 'u1L10BSRzVs_2_wERnQNuwQbM_2_original_input_2.png', 'xZjp2wTRB00_2_xf9OCpcjNxw_1_original_input_1.png', 'sO1UU5w51-4_0_tTzFkD7YJ8g_2_original_input_8.png', 'unJBUjkCJ-s_0_uuCpdpU7Ny0_0_7.png'] and prediction is tensor([1, 0, 1, 1, 0, 0, 1, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yuyl1f-B6iU_1_y8arX_WtdfM_1_original_input_1.png', 'wTsoXbhr0N8_4_wnx2fsN9WP0_1_original_input_5.png', 'vcdcGKDA2ys_0_ualXuiy2SN4_0_original_input_5.png', 'ybIKgqf-Zls_0_y1uIt39-154_2_original_input_7.png', 'szxR2kMIWtI_6_szxR2kMIWtI_6_4.png', 'z1TT_iTuSfU_0_zXVbI88_88E_0_original_input_7.png', 'sRASRj8cG9E_2_vJv2Bemw7MU_4_original_input_9.png', 'xG9J__APvnY_0_yIvIfkuWRSY_1_original_input_6.png'] and prediction is tensor([0, 0, 0, 0, 1, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['tuAgtClLSxE_0_tKVEzPMy9x0_0_original_input_4.png', 'tH6p0SPG5IM_0_tPT-6R-a1W0_0_original_input_2.png', 'szxR2kMIWtI_6_szxR2kMIWtI_6_original_input_5.png', 'rai6unYUpiM_1_t6rrtC12SfM_2_6.png', 'v4JKM1tSlJg_2_w9H1lP-eOoA_0_original_input_0.png', 'z1TT_iTuSfU_0_zXVbI88_88E_0_9.png', 'yVhpvBgKCgc_0_x3XdiNt08sA_0_9.png', 'vvT0O0n6Muc_3_vjSgViWZEjY_2_8.png'] and prediction is tensor([0, 0, 0, 1, 1, 0, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['s8CuBT-MDYE_4_sZqIjquRfck_8_original_input_0.png', 'qpe2zydDOBM_0_r4evQe47qE0_4_original_input_7.png', 'yVhpvBgKCgc_0_x3XdiNt08sA_0_original_input_5.png', 'qpBoMZ8mAiQ_2_qpe2zydDOBM_0_original_input_8.png', 's1jqbrlDefs_0_tH6p0SPG5IM_0_1.png', 'wAu_YWTQ3Po_0_sRASRj8cG9E_2_3.png', 'ta1sycJ415M_2_sZqIjquRfck_8_original_input_1.png', 'rIiQLEa8iZ0_2_r4evQe47qE0_4_6.png'] and prediction is tensor([0, 0, 0, 0, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['u1L10BSRzVs_1_uFxeBopzRGU_3_original_input_1.png', 'wLkWLkZTEAU_1_wAoDSioZkV8_0_original_input_4.png', 'zYtJwZWcRXk_3_zR3A0fzf1Kg_1_9.png', 'qQUcjcS06IY_1_qs8SVi4pA4Y_1_original_input_2.png', 'xgW0lJJsGmc_0_xs-QgU_Msy0_0_0.png', 'vvT0O0n6Muc_3_vjSgViWZEjY_2_original_input_4.png', 'wUU-Y1ffMp0_0_uvxJpaN72WQ_2_original_input_7.png', 'tPT-6R-a1W0_0_tKVEzPMy9x0_0_5.png'] and prediction is tensor([0, 1, 1, 0, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yn5qTgXwcx4_0_y_vhh1BuwDQ_0_5.png', 'xZjp2wTRB00_3_yueeQKJK-wc_0_original_input_4.png', '093EdKDP7Zs_5_8-n1eJ93hLo_1_original_input_1.png', 'r4evQe47qE0_4_rGszRN55VHs_3_9.png', 'w2h4xISxMtc_2_wYt5Mpl1flY_1_1.png', 'yueeQKJK-wc_0_xG9J__APvnY_4_original_input_1.png', 'rgkJiQbfE8Q_3_rPBfL0TlQIc_4_original_input_6.png', 'sRASRj8cG9E_2_vJv2Bemw7MU_4_2.png'] and prediction is tensor([1, 0, 0, 1, 1, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yn5qTgXwcx4_0_y_vhh1BuwDQ_0_original_input_1.png', 'pwsP8rk2ouU_1_qb_sn9hVqh0_0_original_input_4.png', 'szxR2kMIWtI_0_saQyF1FPiqw_0_original_input_8.png', 'wF2J3IRCd58_0_ukdxw_rACaw_0_5.png', 'yXdpfHzOVac_6_xs-QgU_Msy0_0_original_input_0.png', 'ydPPnou6Frs_2_xZjp2wTRB00_3_original_input_1.png', 'x4dffEgdvwU_1_yX3GBqrETss_1_original_input_1.png', 'ueWSYARSUnI_0_uvbyZZFC25g_2_8.png'] and prediction is tensor([1, 0, 0, 1, 0, 0, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['xG9J__APvnY_4_yNtEPE_OCfg_0_original_input_2.png', 'yE1vy3JnftM_0_xhNkjaNQmYk_2_original_input_7.png', 'w9H1lP-eOoA_3_wedLZCKQSrM_2_original_input_0.png', 'ukjUkWWRs9w_0_unUjJkjyh9E_2_4.png', 'r0WUkAPxFhw_8_wAu_YWTQ3Po_0_2.png', 'rai6unYUpiM_1_t6rrtC12SfM_2_original_input_5.png', 'vcdcGKDA2ys_0_ualXuiy2SN4_0_2.png', 'tApBREQnlcc_3_t6rrtC12SfM_2_original_input_0.png'] and prediction is tensor([0, 0, 1, 1, 1, 0, 1, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score array is tensor([2.7327, 2.7399, 1.7907, 1.0890, 2.9628, 2.9555, 1.3655, 3.6005, 2.0438,\n",
      "        1.6391], device='cuda:0')\n",
      "And filename is ['xQHrfDT6XJ0_0_ySy2dIJ889c_0_0.png', 'qZt442xDsAw_0_r4evQe47qE0_3_5.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_2.png', 'wedLZCKQSrM_0_w2h4xISxMtc_2_1.png', 'yd17VGQZJs8_0_yj04mRa-AMA_1_9.png', 'tApBREQnlcc_3_t6rrtC12SfM_2_3.png', 'xZjp2wTRB00_3_yueeQKJK-wc_0_0.png', 't2T_pvkLm3s_1_s8CuBT-MDYE_4_original_input_7.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['xQHrfDT6XJ0_0_ySy2dIJ889c_0_0.png', 'qZt442xDsAw_0_r4evQe47qE0_3_5.png', 'xl0CyL44YzA_1_yq2rKzQrYVg_2_2.png', 'wedLZCKQSrM_0_w2h4xISxMtc_2_1.png', 'yd17VGQZJs8_0_yj04mRa-AMA_1_9.png', 'tApBREQnlcc_3_t6rrtC12SfM_2_3.png', 'xZjp2wTRB00_3_yueeQKJK-wc_0_0.png', 't2T_pvkLm3s_1_s8CuBT-MDYE_4_original_input_7.png'] and prediction is tensor([0, 1, 1, 1, 0, 1, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['wlSJLDlCxlY_2_wlSJLDlCxlY_2_4.png', 'uvxJpaN72WQ_2_u80gOsgFcIs_4_3.png', 'tPT-6R-a1W0_0_tKVEzPMy9x0_0_original_input_6.png', 'qqvLTSOwm8M_0_qQUcjcS06IY_1_original_input_1.png', 'wOBTdvyWWNk_0_uzWFtZBcOIA_3_original_input_0.png', 'qqvLTSOwm8M_0_qQUcjcS06IY_1_6.png', 'vOR4lA-aOfI_1_unUjJkjyh9E_2_original_input_0.png', 'ukdxw_rACaw_4_wTsoXbhr0N8_4_0.png'] and prediction is tensor([1, 1, 0, 0, 0, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yA13FQmjfNQ_0_yjsNEL7DQuY_5_original_input_4.png', 'zIowgsPebcE_4_zgkHFG3upQ0_3_6.png', 'z10FlGaSfsw_8_zlKkyKw7xfw_0_8.png', 'vJv2Bemw7MU_4_r0WUkAPxFhw_8_4.png', '093EdKDP7Zs_5_8-n1eJ93hLo_1_1.png', 'ukdxw_rACaw_0_ueWSYARSUnI_0_6.png', 'vlY2k3e3Xfs_0_wedLZCKQSrM_2_original_input_0.png', 'qs8SVi4pA4Y_1_rI7GbgEhwoY_1_original_input_7.png'] and prediction is tensor([0, 1, 1, 1, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yE1vy3JnftM_0_xhNkjaNQmYk_2_1.png', 'r7B491zbxnw_0_pXJ6HOsm_iY_0_original_input_6.png', 'yyMFVp5Ut0Y_8_zYtJwZWcRXk_5_4.png', 'wERnQNuwQbM_3_vOR4lA-aOfI_1_original_input_3.png', 'xffuIgiRUZw_6_ySy2dIJ889c_0_7.png', 'uafR2rn51YA_0_vJv2Bemw7MU_4_original_input_4.png', 'yuyl1f-B6iU_0_y4PqkuhOzNc_1_0.png', 'xiGGcoQG8JI_0_ysNczfg7n8w_0_7.png'] and prediction is tensor([1, 0, 1, 0, 1, 1, 1, 1])\n",
      "score array is tensor([3.1228, 2.2718, 3.0007, 2.8074, 2.0200, 3.6235, 2.5422, 2.4954, 3.0237,\n",
      "        2.3632], device='cuda:0')\n",
      "And filename is ['wAoDSioZkV8_0_wbLsNxqHyeA_1_original_input_2.png', 'vm5DPRNMfvQ_0_uE-bz8xv0Mw_0_original_input_0.png', 'v-ZKI-eqKtg_1_wb2iK723AkI_3_original_input_7.png', 'xQHrfDT6XJ0_4_yrkevMmkVMU_4_original_input_9.png', 'v-ZKI-eqKtg_1_wb2iK723AkI_3_0.png', 'w9H1lP-eOoA_0_wb2iK723AkI_3_original_input_6.png', 'yxucvKpCtDo_2_yarXrvfBZ9c_0_original_input_4.png', 'sNCVZ7VmJ98_1_sNCVZ7VmJ98_1_6.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['wAoDSioZkV8_0_wbLsNxqHyeA_1_original_input_2.png', 'vm5DPRNMfvQ_0_uE-bz8xv0Mw_0_original_input_0.png', 'v-ZKI-eqKtg_1_wb2iK723AkI_3_original_input_7.png', 'xQHrfDT6XJ0_4_yrkevMmkVMU_4_original_input_9.png', 'v-ZKI-eqKtg_1_wb2iK723AkI_3_0.png', 'w9H1lP-eOoA_0_wb2iK723AkI_3_original_input_6.png', 'yxucvKpCtDo_2_yarXrvfBZ9c_0_original_input_4.png', 'sNCVZ7VmJ98_1_sNCVZ7VmJ98_1_6.png'] and prediction is tensor([0, 0, 0, 0, 1, 1, 0, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['sZDvg03saDA_4_sZDvg03saDA_4_original_input_6.png', 'pRrc0B9FlOU_4_rGszRN55VHs_3_original_input_4.png', 's8raRFJ7z9w_0_sqfDnCME350_0_original_input_9.png', 'y_vhh1BuwDQ_0_wuJZuJPVRUk_0_original_input_8.png', 'w2h4xISxMtc_2_wYt5Mpl1flY_1_original_input_0.png', 'qs8SVi4pA4Y_1_rI7GbgEhwoY_1_5.png', 'u1L10BSRzVs_1_uFxeBopzRGU_3_1.png', 'vjQ-AzDvPWE_0_vOR4lA-aOfI_1_1.png'] and prediction is tensor([0, 0, 0, 0, 0, 1, 1, 1])\n",
      "score array is tensor([1.6952, 3.4129, 3.6867, 1.7231, 3.3881, 3.1832, 2.3627, 1.8603, 2.4327,\n",
      "        2.8218], device='cuda:0')\n",
      "And filename is ['yXdpfHzOVac_9_yZVyZN2kZKY_0_4.png', 'yj04mRa-AMA_1_yX3GBqrETss_1_original_input_7.png', 'yarXrvfBZ9c_0_xQHrfDT6XJ0_4_7.png', 'xQHrfDT6XJ0_4_yrkevMmkVMU_4_4.png', 'xQHrfDT6XJ0_0_ySy2dIJ889c_0_original_input_1.png', 'xe1sO7aw6J4_7_y1uIt39-154_0_original_input_6.png', 'ualXuiy2SN4_0_wlSJLDlCxlY_2_1.png', 'qwB6fx8caeY_0_qb_sn9hVqh0_0_original_input_9.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yXdpfHzOVac_9_yZVyZN2kZKY_0_4.png', 'yj04mRa-AMA_1_yX3GBqrETss_1_original_input_7.png', 'yarXrvfBZ9c_0_xQHrfDT6XJ0_4_7.png', 'xQHrfDT6XJ0_4_yrkevMmkVMU_4_4.png', 'xQHrfDT6XJ0_0_ySy2dIJ889c_0_original_input_1.png', 'xe1sO7aw6J4_7_y1uIt39-154_0_original_input_6.png', 'ualXuiy2SN4_0_wlSJLDlCxlY_2_1.png', 'qwB6fx8caeY_0_qb_sn9hVqh0_0_original_input_9.png'] and prediction is tensor([1, 0, 1, 0, 0, 0, 1, 0])\n",
      "score array is tensor([3.1295, 2.1642, 2.1686, 1.5784, 3.7912, 1.9106, 2.2660, 2.3871, 2.6484,\n",
      "        3.4526], device='cuda:0')\n",
      "And filename is ['yNtEPE_OCfg_0_y_vhh1BuwDQ_0_original_input_4.png', 'yd17VGQZJs8_0_yj04mRa-AMA_1_original_input_5.png', 'r4evQe47qE0_3_qQUcjcS06IY_1_original_input_9.png', '18KtBf-4sPs_0_5b96N4ciJRY_3_9.png', 'xtYnIghWxJ0_0_ta1sycJ415M_1_original_input_3.png', 'pbuuZv60C4Q_6_r81Q3Gnrihc_1_5.png', 'r7B491zbxnw_0_pXJ6HOsm_iY_0_0.png', 'zIowgsPebcE_4_zgkHFG3upQ0_3_original_input_5.png']\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['yNtEPE_OCfg_0_y_vhh1BuwDQ_0_original_input_4.png', 'yd17VGQZJs8_0_yj04mRa-AMA_1_original_input_5.png', 'r4evQe47qE0_3_qQUcjcS06IY_1_original_input_9.png', '18KtBf-4sPs_0_5b96N4ciJRY_3_9.png', 'xtYnIghWxJ0_0_ta1sycJ415M_1_original_input_3.png', 'pbuuZv60C4Q_6_r81Q3Gnrihc_1_5.png', 'r7B491zbxnw_0_pXJ6HOsm_iY_0_0.png', 'zIowgsPebcE_4_zgkHFG3upQ0_3_original_input_5.png'] and prediction is tensor([0, 0, 0, 1, 0, 1, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['tmSjjPqbakU_0_tKVEzPMy9x0_0_original_input_0.png', 'zIowgsPebcE_7_zR3A0fzf1Kg_1_4.png', 'wERnQNuwQbM_2_wAoDSioZkV8_0_original_input_6.png', 'vlY2k3e3Xfs_0_wedLZCKQSrM_2_4.png', 'szxR2kMIWtI_1_tTzFkD7YJ8g_2_original_input_2.png', 'tmSjjPqbakU_0_tKVEzPMy9x0_0_0.png', 'wTsoXbhr0N8_4_wnx2fsN9WP0_1_2.png', 'yq2rKzQrYVg_2_yX3GBqrETss_1_4.png'] and prediction is tensor([0, 1, 0, 1, 0, 1, 1, 1])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "filename is ['vOJztGgaiaY_1_vOTS31zUyBg_0_original_input_6.png', 'sO1UU5w51-4_1_tcyOetKW0BE_3_4.png', 'rIiQLEa8iZ0_0_rI7GbgEhwoY_1_original_input_0.png', 'vjQ-AzDvPWE_0_vOR4lA-aOfI_1_original_input_3.png', 'rai6unYUpiM_0_rai6unYUpiM_0_4.png', 'zA9-nGZacyg_0_zD_OLZWppaA_1_original_input_3.png', 'vRCCZzmv5xs_1_ugyYHAxIMrc_0_original_input_5.png', 'ptJq_5_lF6Q_0_qZ_ozYF3eug_0_original_input_2.png'] and prediction is tensor([0, 1, 0, 0, 0, 0, 0, 0])\n",
      "actual value of labels read from disc is tensor([1, 1, 1, 1])\n",
      "filename is ['yxucvKpCtDo_2_yarXrvfBZ9c_0_3.png', '0qjxH8dxbFE_0_9W5-B_oWec8_3_original_input_7.png', 'xtYnIghWxJ0_0_ta1sycJ415M_1_3.png', 'zYtJwZWcRXk_5_yyMFVp5Ut0Y_8_original_input_0.png'] and prediction is tensor([1, 0, 0, 0])\n",
      "maximum score is 3.791227340698242\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    # train_losses, train_scores = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(model, 'cuda', valid_loader)\n",
    "    # Reduce learning-rate by a factor of 1/10 after every 10 epochs\n",
    "    # avoid this step as Adam is being used\n",
    "    #adjust_learning_rate(optimizer=optimizer, learning_rate=learning_rate, epoch=epoch)\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
